{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to repo directory if we are in the notebook directory\n",
    "import os\n",
    "if os.getcwd().endswith(\"utils\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import multiprocess\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import editdistance\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "from nnsplit import NNSplit\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions/variables setup ###\n",
    "\n",
    "notebook_path = os.path.dirname(os.path.realpath(next(\n",
    "    glob.iglob(os.path.join(os.getcwd(), \"**\", \"training.ipynb\")), \"./training.ipynb\"\n",
    ")))\n",
    "\n",
    "corpus_path = os.path.join(notebook_path, \"..\", \"corpus\")\n",
    "os.makedirs(corpus_path, exist_ok=True)\n",
    "\n",
    "data_path = os.path.join(notebook_path, \"..\", \"data\")\n",
    "os.makedirs(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"dictionary.txt\")) as f:\n",
    "    dictionary = set(line.strip().lower() for line in f.readlines())\n",
    "dictionary_list = sorted(dictionary)\n",
    "\n",
    "def clean_word(word):\n",
    "    word_lower = word.lower()\n",
    "    return min(dictionary_list, key=lambda w: editdistance.eval(w, word_lower))\n",
    "    # return match_capitalization(word, new_word)\n",
    "\n",
    "def clean_worker(queue, words):\n",
    "    for word in words:\n",
    "        queue.put({ word: clean_word(word) })\n",
    "\n",
    "def clean_text(text):\n",
    "    # Find all words that aren't in the dictionary and clean those\n",
    "\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    words = set(word for word in tokens if word not in \"...\" + string.punctuation)\n",
    "    words.difference_update(dictionary)\n",
    "\n",
    "    # Get word mappings on multiple cores to speed up process\n",
    "    words_list = list(words)\n",
    "    results_queue = multiprocess.Queue()\n",
    "    procs = []\n",
    "    n_procs = max(1, multiprocess.cpu_count() - 1)\n",
    "    for i in range(n_procs):\n",
    "        proc = multiprocess.Process(target=clean_worker, args=(results_queue, words_list[i*len(words_list)//n_procs : (i+1)*len(words_list)//n_procs],))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "    word_cleaned = {}\n",
    "    # Pull from the multiprocess queue while workers are cleaning words\n",
    "    while all(proc.is_alive() for proc in procs):\n",
    "        while not results_queue.empty():\n",
    "            cleaned = results_queue.get()\n",
    "            word_cleaned.update(cleaned)\n",
    "        time.sleep(1)\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "\n",
    "    # Now replace the words that aren't present in the dictionary\n",
    "    new_tokens = [word_cleaned.get(word, word) for word in tokens]\n",
    "    return TreebankWordDetokenizer().detokenize(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting corpus name for training on\n",
    "corpus_name = \"sonicsez\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Model Training\n",
    "\n",
    "**Note**: replace `corpus_name` above Punctuation Restoration Training with the filename of the corpus you want to train on! (without the path and extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "with open(os.path.join(corpus_path, f\"{corpus_name}.txt\")) as f:\n",
    "    contents = [line.strip() for line in f.readlines()]\n",
    "\n",
    "text = clean_text(\"\\n\".join(contents))\n",
    "\n",
    "# Strip stray apostrophes\n",
    "text = re.sub(r\"(?:^'|(?<![a-z])'(?![a-z])|'$)\", \"\", text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the cleaned text for later usage since the process is intensive\n",
    "with open(os.path.join(corpus_path, f\"{corpus_name}.cleaned.txt\"), \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load the cleaned text on a later rerun\n",
    "with open(os.path.join(corpus_path, f\"{corpus_name}.cleaned.txt\")) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate corpus by newlines\n",
    "corpus = [sentence.strip() for sentence in text.split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer including basic punctuation\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "print(f\"Size of vocab: {len(tokenizer.word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert corpus into index sequences\n",
    "extracted_sequences = tokenizer.texts_to_sequences(corpus)\n",
    "max_seq_len = 20\n",
    "\n",
    "# Flatten extracted sequences and build new sequences from that\n",
    "flattened_sequences = [token for seq in extracted_sequences for token in seq]\n",
    "corpus_len = len(flattened_sequences)\n",
    "step_size = round(max(1, corpus_len / 50000))\n",
    "\n",
    "# Create a sequences list stepping every few words in the corpus\n",
    "sequences = []\n",
    "\n",
    "for i in range(0, len(flattened_sequences), step_size):\n",
    "    seq = flattened_sequences[i : i + max_seq_len]\n",
    "    if len(seq) > 2:\n",
    "        sequences.append(seq)\n",
    "\n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "idx_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to make them all the same length, so we can input them into the RNN later\n",
    "padded_sequences = np.array(pad_sequences(sequences, maxlen=max_seq_len, padding=\"pre\"))\n",
    "\n",
    "# Now set up sequence -> word output vectors for training\n",
    "predictors, labels = padded_sequences[:, :-1], padded_sequences[:, -1]\n",
    "\n",
    "# One-hot encode the outputs (bag-of-words)\n",
    "labels = utils.to_categorical(labels, num_classes=vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model to train on\n",
    "seq_len = max_seq_len - 1\n",
    "hidden_nodes = (seq_len + 1) * 2\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(vocab_len, 20, input_length=seq_len),\n",
    "    layers.LSTM(hidden_nodes),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(vocab_len, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=\"adam\")\n",
    "\n",
    "# Save initial model data for reloading, and also checkpointing for saving model weights\n",
    "model_path = os.path.join(notebook_path, f\"{corpus_name}.model.h5\")\n",
    "checkpoint = callbacks.ModelCheckpoint(model_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
    "model_callbacks = [checkpoint]\n",
    "\n",
    "with open(os.path.join(notebook_path, f\"{corpus_name}.wordmap.json\"), \"w\") as f:\n",
    "   f.write(json.dumps(tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def sample(prediction, temperature = 1.0):\n",
    "    prediction = np.asarray(prediction, dtype=np.float64)\n",
    "    prediction = np.log(prediction) / temperature\n",
    "    exp_prediction = np.exp(prediction)\n",
    "    prediction = exp_prediction / np.sum(exp_prediction)\n",
    "    probabilities = np.random.multinomial(1, prediction[0,:], 1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "model.fit(\n",
    "    predictors, labels, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    callbacks=model_callbacks, validation_split=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nTesting output from training:\")\n",
    "seed_idx = random.randrange(0, len(predictors))\n",
    "pattern = predictors[seed_idx].tolist()\n",
    "current_sentence = \" \".join(idx_word.get(idx, \"\") for idx in pattern)\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    generated_text = \"\"\n",
    "    current_pattern = pattern.copy()\n",
    "    \n",
    "    for i in range(25):\n",
    "        # Run prediction through the model and get the index of the prediction\n",
    "        input_sequence = np.reshape(current_pattern, (1, len(current_pattern)))\n",
    "        prediction = model.predict(input_sequence, verbose=0)\n",
    "        idx = sample(prediction, temperature)\n",
    "        result = idx_word.get(idx, \"\")\n",
    "        generated_text += result + \" \"\n",
    "\n",
    "        current_pattern.append(idx)\n",
    "        current_pattern = current_pattern[1:]\n",
    "    \n",
    "    print(f\"- For temperature {temperature}:\")\n",
    "    print(f\"--- Input seed: {current_sentence}\")\n",
    "    print(f\"---  Generated: {generated_text}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures with sentence segmentation\n",
    "\n",
    "model_path = os.path.join(notebook_path, f\"{corpus_name}.model.h5\")\n",
    "model = models.load_model(model_path)\n",
    "with open(os.path.join(notebook_path, f\"{corpus_name}.wordmap.json\")) as f:\n",
    "    # Fix JSON requiring keys to be strings by making them ints again\n",
    "    idx_word = {int(key): val for key, val in json.load(f).items()}\n",
    "\n",
    "with open(os.path.join(notebook_path, \"punctuation.probabilities.json\")) as f:\n",
    "    punc_probabilities = json.load(f)\n",
    "sent_splitter = NNSplit.load(\"en\")\n",
    "\n",
    "def sample(prediction, temperature = 1.0):\n",
    "    prediction = np.asarray(prediction, dtype=np.float64)\n",
    "    prediction = np.log(prediction) / temperature\n",
    "    exp_prediction = np.exp(prediction)\n",
    "    prediction = exp_prediction / np.sum(exp_prediction)\n",
    "    probabilities = np.random.multinomial(1, prediction[0,:], 1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "max_word_index = max(key for key in idx_word.keys())\n",
    "pattern = [random.randint(1, max_word_index) for _ in range(19)]\n",
    "current_sentence = \" \".join(idx_word.get(idx, \"\") for idx in pattern)\n",
    "for temperature in np.arange(0.2, 1.3, 0.1):\n",
    "    generated_text = \"\"\n",
    "    current_pattern = pattern.copy()\n",
    "    \n",
    "    for i in range(50):\n",
    "        # Run prediction through the model and get the index of the prediction\n",
    "        input_sequence = np.reshape(current_pattern, (1, len(current_pattern)))\n",
    "        prediction = model.predict(input_sequence, verbose=0)\n",
    "        idx = sample(prediction, temperature)\n",
    "        result = idx_word.get(idx, \"\")\n",
    "        generated_text += result + \" \"\n",
    "\n",
    "        current_pattern.append(idx)\n",
    "        current_pattern = current_pattern[1:]\n",
    "    \n",
    "    print(f\"- For temperature {temperature}:\")\n",
    "    print(f\"--- Input seed: {current_sentence}\")\n",
    "    print(f\"---  Generated: {generated_text}\")\n",
    "    print( \"---  Sentences:\")\n",
    "    for sentence in sent_splitter.split([generated_text])[0]:\n",
    "        sentence = str(sentence).strip()\n",
    "        last_part_of_speech = nltk.pos_tag(nltk.word_tokenize(sentence))[-1][1]\n",
    "        punc_marks = punc_probabilities.get(last_part_of_speech, {})\n",
    "        if len(punc_marks) > 0:\n",
    "            punc_mark_probs = [(mark, pr) for mark, pr in punc_marks.items()]\n",
    "            marks = [mark for mark, pr in punc_mark_probs]\n",
    "            probs = [pr for mark, pr in punc_mark_probs]\n",
    "            punc_mark = random.choices(marks, probs, k=1)[0]\n",
    "        else:\n",
    "            punc_mark = \"\"\n",
    "        # Rudimentary sentence constructor, will change\n",
    "        sentence = sentence[0].upper() + sentence[1:] + punc_mark\n",
    "        print(f\"----- {sentence}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually save the latest version of the model. Not needed if using checkpoints above\n",
    "# model.save(model_path)\n",
    "\n",
    "# Re-create the model with the below code:\n",
    "# model = models.load_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Restoration Training\n",
    "\n",
    "**Note**: replace `corpus_name` above this section with the filename of the corpus you want to train on! (without the path and extension)\n",
    "\n",
    "Right now, this is very basic, only calculating the probability a part of speech has a punctuation mark after it. After sentence segmentation, these probabilities will be used to punctuate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "with open(os.path.join(corpus_path, f\"{corpus_name}.txt\")) as f:\n",
    "    contents = [line.strip() for line in f.readlines()]\n",
    "\n",
    "text = clean_text(\"\\n\".join(contents))\n",
    "\n",
    "# Strip stray apostrophes\n",
    "text = re.sub(r\"(?:^'|(?<![a-z])'(?![a-z])|'$)\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "# Remove punctuation besides the ones we care about\n",
    "remove_regex = \"[\" + re.escape(\"\".join(mark for mark in string.punctuation if mark not in '!\"\\',.:;?')) + \"]\"\n",
    "text = re.sub(remove_regex, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_parts = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "punc_instances = {}\n",
    "for word1, word2 in zip(speech_parts, speech_parts[1:]):\n",
    "    part1 = word1[1]\n",
    "    text2 = word2[0] # Should contain the actual punctuation mark if it is punctuation\n",
    "    part2 = word2[1]\n",
    "    if part1 not in '.,:': # If first token of pair is an actual word, count that instance\n",
    "        if part1 not in punc_instances:\n",
    "            punc_instances[part1] = {}\n",
    "        if part2 in '.,:': # If second token of pair is punctuation, add pair to known pairs\n",
    "            punc_instances[part1][text2] = punc_instances[part1].get(text2, 0) + 1\n",
    "\n",
    "# Clean unused keys in the instances dict, and calculate probabilities\n",
    "punc_probabilities = {}\n",
    "for part in punc_instances.keys():\n",
    "    if len(punc_instances[part]) > 0:\n",
    "        sum_instances = sum(punc_instances[part].values())\n",
    "        punc_probabilities[part] = { punc: round(punc_instances[part][punc] / sum_instances, 10) for punc in punc_instances[part]}\n",
    "\n",
    "with open(os.path.join(notebook_path, \"punctuation.probabilities.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(punc_probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert h5 to onnx Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().endswith(\"utils\"):\n",
    "    os.chdir(\"..\")\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "import onnx\n",
    "import tf2onnx\n",
    "\n",
    "notebook_path = os.path.dirname(os.path.realpath(next(\n",
    "    glob.iglob(os.path.join(os.getcwd(), \"**\", \"training.ipynb\")), \"./training.ipynb\"\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in glob.glob(os.path.join(notebook_path, \"*.h5\")):\n",
    "    model = models.load_model(fname)\n",
    "    input_signature = [tf.TensorSpec([1, 19], tf.float32, name=\"input\")]\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature)\n",
    "    model_name = os.path.splitext(os.path.basename(fname))[0]\n",
    "    onnx.save(onnx_model, os.path.join(notebook_path, f\"{model_name}.onnx\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93ec6c4d5a6da901becf8644899d0515c414d17031cb77ccce7b77fe646f214f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
