{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to repo directory if we are in the notebook directory\n",
    "import os\n",
    "if os.getcwd().endswith(\"utils\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import html2text\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, ElementNotInteractableException, NoSuchElementException\n",
    "import shutil\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "\n",
    "from utils.profanity import ProfanityFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper variables/functions setup\n",
    "\n",
    "notebook_path = os.path.dirname(os.path.realpath(next(\n",
    "    glob.iglob(os.path.join(os.getcwd(), \"**\", \"scrapers.ipynb\")), \"./scrapers.ipynb\"\n",
    ")))\n",
    "\n",
    "data_path = os.path.realpath(os.path.join(notebook_path, \"..\", \"data\"))\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "corpus_path = os.path.realpath(os.path.join(notebook_path, \"..\", \"corpus\"))\n",
    "os.makedirs(corpus_path, exist_ok=True)\n",
    "\n",
    "profanity_filter = ProfanityFilter(censor_pool=\"@#%^*\")\n",
    "profanity_pool_regex = re.compile(re.escape(profanity_filter.censor_pool), flags=re.IGNORECASE)\n",
    "whitespace_shrink_regex = re.compile(r\"[\\r\\n]+\")\n",
    "punctuation_regex = re.compile(r\"\"\"[\\.,!?:;\"'\\(\\)\\[\\]]\"\"\")\n",
    "url_regex = re.compile(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", flags=re.IGNORECASE)\n",
    "\n",
    "def prepare_scraped_text(text):\n",
    "    # Prepare text retrieved from scraper for model usage\n",
    "    return profanity_filter.censor(\n",
    "        profanity_pool_regex.sub(\"\",\n",
    "            whitespace_shrink_regex.sub(\"\\n\", unidecode(text.strip(\"\\n\")))\n",
    "        )\n",
    "    )\n",
    "\n",
    "def backup_file(filepath):\n",
    "    # Backup original file and save in original's location\n",
    "    file_directory = os.path.dirname(os.path.realpath(filepath))\n",
    "    file_name = os.path.basename(filepath)\n",
    "    backup_filepath = os.path.join(file_directory, \"bak\", file_name)\n",
    "    os.makedirs(os.path.join(file_directory, \"bak\"), exist_ok=True)\n",
    "    os.rename(filepath, backup_filepath)\n",
    "\n",
    "def cleanup_scraped_text(filepath):\n",
    "    with open(filepath) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Run a series of substitutions and other fixes to clean text\n",
    "    text = url_regex.sub(\"\", text)\n",
    "    text = text.replace(\"~\", \"\")\n",
    "    text = \"\\n\".join(line.strip() for line in text.split(\"\\n\"))\n",
    "    text = whitespace_shrink_regex.sub(\"\\n\", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r'^\"\\s+', '\"', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+\"$', '\"', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(^[^\"]+\"[^\"]+)\\s+\"', r'\\1\"', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\"\\s+(?=[^\"]+\"[^\"]+$)', '\"', text, flags=re.MULTILINE)\n",
    "    text = text.replace(\"\\\\.\", \".\").replace(\"\\\\-\", \"-\").replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    # Fix for weird behavior from replacing names with pronouns\n",
    "    text = re.sub(r'(\\b(?:he|him|his|she|her|hers|they|them|their|theirs)\\b\\s*)+', r\"\\1\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    backup_file(filepath)\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "blocker_regex = re.compile(\"DDoS protection\", flags=re.IGNORECASE)\n",
    "\n",
    "html2text_handler = html2text.HTML2Text()\n",
    "html2text_handler.ignore_links = True\n",
    "html2text_handler.ignore_images = True\n",
    "html2text_handler.body_width = 2147483647\n",
    "html2text_handler.emphasis_mark = \"\"\n",
    "html2text_handler.strong_mark = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Webdriver Setup ###\n",
    "\n",
    "scraperfiles_path = os.path.join(notebook_path, \"scraper-files\")\n",
    "os.makedirs(scraperfiles_path, exist_ok=True)\n",
    "\n",
    "# Get UBlock Origin and add to Chrome to speed up page load times\n",
    "ublock_origin_directory = os.path.join(scraperfiles_path, \"ublock_origin\")\n",
    "if not os.path.exists(ublock_origin_directory):\n",
    "    ublock_origin_crx_file = os.path.join(scraperfiles_path, \"cjpalhdlnbpafiamejdnhcphjbkeiagm.zip\")\n",
    "    if not os.path.exists(ublock_origin_crx_file):\n",
    "        chrome_version = \"100.0.4896.127\"\n",
    "        extension_id = \"cjpalhdlnbpafiamejdnhcphjbkeiagm\"\n",
    "        ublock_download_url = f\"https://clients2.google.com/service/update2/crx?response=redirect&prodversion={chrome_version}&acceptformat=crx2,crx3&x=id%3D{extension_id}%26uc\"\n",
    "        extension_response = requests.get(ublock_download_url)\n",
    "        if extension_response.ok:\n",
    "            with open(ublock_origin_crx_file, \"wb\") as f:\n",
    "                f.write(extension_response.content)\n",
    "    # Make sure file exists from above, otherwise just ignore it\n",
    "    if os.path.exists(ublock_origin_crx_file):\n",
    "        os.makedirs(ublock_origin_directory, exist_ok=True)\n",
    "        shutil.unpack_archive(ublock_origin_crx_file, ublock_origin_directory)\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--user-data-dir=/tmp/scrape-chromium-profile\")\n",
    "options.add_argument(\"--no-first-run --no-service-autorun --password-store=basic\")\n",
    "if os.path.exists(ublock_origin_directory):\n",
    "    options.add_argument(f\"--load-extension={ublock_origin_directory}\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from the nice work done here: https://dlsun.github.io/pods/data/names/\n",
    "base_url = \"https://dlsun.github.io/pods/data/names/yob{}.txt\"\n",
    "\n",
    "# Create dicts mapping names to number of occurrences, to calculate probabilities later\n",
    "names_man = defaultdict(lambda: 0)\n",
    "names_woman = defaultdict(lambda: 0)\n",
    "\n",
    "for year in range(1980, 2011):\n",
    "    driver.get(base_url.format(year))\n",
    "    # Remove added html info to just get list\n",
    "    names_list = whitespace_shrink_regex.sub(\"\\n\",\n",
    "        html2text_handler.handle(driver.page_source)\n",
    "    ).replace(\" \",\"\").strip(\" \\n\")\n",
    "    # Split name data like basic CSV\n",
    "    name_data = [\n",
    "        [col for col in row.split(\",\")]\n",
    "        for row in names_list.split(\"\\n\") if len(row.strip()) > 0\n",
    "    ]\n",
    "    for name, gender, n_times in name_data:\n",
    "        if gender == \"F\":\n",
    "            names_woman[name] += int(n_times)\n",
    "        else:\n",
    "            names_man[name] += int(n_times)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Create list of names in both, to get unisex names\n",
    "names_nb = defaultdict(lambda: 0)\n",
    "for name in names_man.keys():\n",
    "    if name in names_woman:\n",
    "        names_nb[name] = names_man[name] + names_woman[name]\n",
    "\n",
    "# Finally, calculate probabilities\n",
    "total_names_man = sum(names_man.values())\n",
    "total_names_woman = sum(names_woman.values())\n",
    "total_names_nb = sum(names_nb.values())\n",
    "pr_names_man = {name: count / total_names_man for name,count in names_man.items()}\n",
    "pr_names_woman = {name: count / total_names_woman for name,count in names_woman.items()}\n",
    "pr_names_nb = {name: count / total_names_nb for name,count in names_nb.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round and re-normalize the probabilities\n",
    "def names_renormalize(pr_names):\n",
    "    for name in pr_names.keys():\n",
    "        pr_names[name] = round(pr_names[name], 9)\n",
    "    adjust_amount = 1 - sum(pr_names.values())\n",
    "    # Adjust highest probability by that amount\n",
    "    most_common_key = max(pr_names.items(), key=lambda item: item[1])[0]\n",
    "    pr_names[most_common_key] += adjust_amount\n",
    "names_renormalize(pr_names_man)\n",
    "names_renormalize(pr_names_woman)\n",
    "names_renormalize(pr_names_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to files\n",
    "with open(os.path.join(data_path, \"names.m.txt\"), \"w\") as f:\n",
    "    for name, pr in sorted(pr_names_man.items(), key=lambda item: -item[1]):\n",
    "        f.write(f\"{name},{pr:.9f}\\n\")\n",
    "with open(os.path.join(data_path, \"names.f.txt\"), \"w\") as f:\n",
    "    for name, pr in sorted(pr_names_woman.items(), key=lambda item: -item[1]):\n",
    "        f.write(f\"{name},{pr:.9f}\\n\")\n",
    "with open(os.path.join(data_path, \"names.x.txt\"), \"w\") as f:\n",
    "    for name, pr in sorted(pr_names_nb.items(), key=lambda item: -item[1]):\n",
    "        f.write(f\"{name},{pr:.9f}\\n\")\n",
    "\n",
    "all_names = set(pr_names_man.keys()).union(pr_names_woman.keys())\n",
    "with open(os.path.join(data_path, \"names.all.txt\"), \"w\") as f:\n",
    "    for name in sorted(all_names):\n",
    "        f.write(name + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fanfic Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "url = f\"https://www.fanfiction.net/game/Sonic-the-Hedgehog/?&p=1\"\n",
    "driver.get(url)\n",
    "\n",
    "for i in range(1000):\n",
    "    while blocker_regex.search(driver.page_source):\n",
    "        time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    for div in soup.select(\"div.z-list:has(a.stitle)\"):\n",
    "        # Check language to see if it's in English\n",
    "        info_div = div.select_one(\"div.xgray\")\n",
    "        if \"English\" in info_div.get_text():\n",
    "            stitle = div.select_one(\"a.stitle\")\n",
    "            titles.append(prepare_scraped_text(stitle.get_text()))\n",
    "    \n",
    "    time.sleep(2+2*random.random())\n",
    "    \n",
    "    try:\n",
    "        next_link = driver.find_element(By.LINK_TEXT, \"Next »\")\n",
    "        next_link.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Reached end of pages, exiting\")\n",
    "        break # Leave loop when we can't go to next page anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(corpus_path, \"fanfics.titles.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BYTE_COUNT = 128 * 1024 ** 2 # Imposing 128 MiB limit\n",
    "bodies = []\n",
    "body_byte_count = 0\n",
    "base_url = \"https://www.fanfiction.net/game/Sonic-the-Hedgehog/\"\n",
    "driver.get(base_url + \"?&p=1\")\n",
    "time.sleep(2+2*random.random())\n",
    "\n",
    "# Handle cookie accept dialog if it pops up\n",
    "try:\n",
    "    driver.find_element(By.XPATH, f\"//div[@onclick='_cookieAccept()']\").click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "for i in range(500):\n",
    "    while blocker_regex.search(driver.page_source):\n",
    "        time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    for div in soup.select(\"div.z-list:has(a.stitle)\"):\n",
    "        time.sleep(2+2*random.random())\n",
    "        \n",
    "        while blocker_regex.search(driver.page_source):\n",
    "            time.sleep(5)\n",
    "            \n",
    "        # Check language to see if it's in English\n",
    "        info_div = div.select_one(\"div.xgray\")\n",
    "        if \"English\" not in info_div.get_text():\n",
    "            continue\n",
    "        # Now open the link\n",
    "        stitle = div.select_one(\"a.stitle\")\n",
    "        stitle_href = stitle.get(\"href\", None)\n",
    "        if stitle_href is None:\n",
    "            continue\n",
    "        \n",
    "        stitle_element = driver.find_element(By.XPATH, f\"//a[@href='{stitle_href}']\")\n",
    "        stitle_element.click()\n",
    "\n",
    "        while blocker_regex.search(driver.page_source):\n",
    "            time.sleep(5)\n",
    "        \n",
    "        child_soup = BeautifulSoup(driver.page_source)\n",
    "        story_div = child_soup.select_one(\"#storytextp\")\n",
    "        story_text = prepare_scraped_text(html2text_handler.handle(str(story_div)))\n",
    "        bodies.append(story_text)\n",
    "        body_byte_count += len(story_text)\n",
    "\n",
    "        if body_byte_count >= MAX_BYTE_COUNT:\n",
    "            break\n",
    "\n",
    "        time.sleep(1+2*random.random())\n",
    "\n",
    "        # Keep going back until at a directory page, in case of blocker check\n",
    "        while not driver.current_url.startswith(base_url):\n",
    "            time.sleep(1)\n",
    "            driver.back()\n",
    "    \n",
    "    if body_byte_count >= MAX_BYTE_COUNT:\n",
    "        print(\"Reached max byte count, exiting\")\n",
    "        break\n",
    "\n",
    "    time.sleep(2+2*random.random())\n",
    "    \n",
    "    try:\n",
    "        next_link = driver.find_element(By.LINK_TEXT, \"Next »\")\n",
    "        next_link.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Reached end of pages, exiting\")\n",
    "        break # Leave loop when we can't go to next page anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(corpus_path, \"fanfics.bodies.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(bodies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OC Descriptions Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_man = []\n",
    "bodies_woman = []\n",
    "bodies_nb = []\n",
    "\n",
    "exclude_names = [\n",
    "    \"An\", \"Bow\", \"Boy\", \"Demon\", \"Do\", \"Don\", \"Had\", \"Her\", \"In\", \"Job\", \"Joy\", \"La\", \"Like\", \"Lo\",\n",
    "    \"Ma\", \"May\", \"Me\", \"Moo\", \"My\", \"No\", \"Nor\", \"Not\", \"Or\", \"Pa\", \"Red\", \"Saw\", \"Say\", \"Sea\",\n",
    "    \"See\", \"Set\", \"Shy\", \"Sir\", \"Ski\", \"Sky\", \"Sly\", \"So\", \"Son\", \"Tea\", \"The\", \"Toy\", \"True\",\n",
    "    \"Van\", \"Win\", \"Won\", \"Woo\", \"Yo\", \"You\", \"Zen\"\n",
    "]\n",
    "\n",
    "def read_names(gender):\n",
    "    with open(os.path.join(corpus_path, f\"names.{gender}.txt\")) as f:\n",
    "        return [\n",
    "            name for line in f.readlines()\n",
    "            # Exclude names that are also spelled like normal words\n",
    "            if (name := line.strip().split(\",\")[0]) not in exclude_names\n",
    "        ]\n",
    "names_man = read_names(\"m\")\n",
    "names_woman = read_names(\"f\")\n",
    "names_nb = read_names(\"x\")\n",
    "with open(os.path.join(corpus_path, f\"names.all.txt\")) as f:\n",
    "    names_all = [\n",
    "        name for line in f.readlines()\n",
    "        # Exclude names that are also spelled like normal words\n",
    "        if (name := line.strip()) not in exclude_names\n",
    "    ]\n",
    "\n",
    "pronoun_man_pattern = r\"\\b(?:he|he'(?:[sd]|ll)|his|him)\\b\"\n",
    "pronoun_woman_pattern = r\"\\b(?:she|she'(?:[sd]|ll)|hers|her)\\b\"\n",
    "pronoun_nb_pattern = r\"\\b(?:they|they'(?:ve|d|ll)|their|theirs|them)\\b\"\n",
    "\n",
    "domain = \"https://sonicfanchara.fandom.com\"\n",
    "base_url = domain + \"/wiki/Category:{}\"\n",
    "\n",
    "def re_count(pattern, string, flags=0):\n",
    "    return sum(1 for _ in re.finditer(pattern, string, flags))\n",
    "\n",
    "def filter_oc_paragraphs(p_text):\n",
    "    # If any of the flags is true, return false\n",
    "    flag_too_short = len(p_text) <= 35\n",
    "    flag_no_sentence = re_count(r\"[\\.!?]\", p_text) == 0\n",
    "    flag_only_key_val = re.search(r\"^[^\\.!?;]{1,25}:\", p_text) is not None and len(p_text) <= 150\n",
    "    return not any((flag_too_short, flag_no_sentence))\n",
    "\n",
    "def get_oc_names(page_title):\n",
    "    lower_page_title = page_title.lower()\n",
    "    if \" the \" in lower_page_title or \",\" in lower_page_title:\n",
    "        search_substr = \" the \" if \" the \" in lower_page_title else \",\"\n",
    "        # Take everything before the word \"the\" and remove punctuation\n",
    "        full_name = punctuation_regex.sub(\"\", page_title[:lower_page_title.find(search_substr)])\n",
    "    else:\n",
    "        # Just consider full name whole thing after removing punctuation\n",
    "        full_name = punctuation_regex.sub(\"\", page_title)\n",
    "    # Start building all names list using all individual names of OC\n",
    "    all_names = full_name.split(\" \")\n",
    "    # Add full name too if all_names split into individual names\n",
    "    # Also add first name + last name if more than 2 names\n",
    "    if len(all_names) > 2:\n",
    "        all_names.insert(0, f\"{all_names[0]} {all_names[-1]}\")\n",
    "    if len(all_names) > 1:\n",
    "        all_names.insert(0, full_name)\n",
    "    return [name.strip() for name in all_names]\n",
    "\n",
    "def infer_oc_gender(oc_names, page_text):\n",
    "    lower_page_text = page_text.lower()\n",
    "    if \"gender: male\" in lower_page_text:\n",
    "        return \"M\"\n",
    "    elif \"gender: female\" in lower_page_text:\n",
    "        return \"F\"\n",
    "    elif \"nonbinary\" in lower_page_text or \"non-binary\" in lower_page_text or \"enby\" in lower_page_text:\n",
    "        return \"X\"\n",
    "    elif any(name.title() in names_all for name in oc_names):\n",
    "        # Do name-based inference\n",
    "        first_oc_name_known = next(name.title() in names_all for name in oc_names)\n",
    "        # Sometimes do non-binary if name is in nb dict\n",
    "        if first_oc_name_known in names_nb and random.random() < 0.1:\n",
    "            return \"X\"\n",
    "        elif first_oc_name_known in names_man:\n",
    "            return \"M\"\n",
    "        else:\n",
    "            return \"F\"\n",
    "    else:\n",
    "        # Resort to count of third person pronouns in the content\n",
    "        pronoun_counts = {\n",
    "            \"M\": re_count(pronoun_man_pattern, page_text, re.IGNORECASE),\n",
    "            \"F\": re_count(pronoun_woman_pattern, page_text, re.IGNORECASE),\n",
    "            \"X\": re_count(pronoun_nb_pattern, page_text, re.IGNORECASE)\n",
    "        }\n",
    "        return max(pronoun_counts.items(), key=lambda item: item[1])[0]\n",
    "\n",
    "def generalize_oc_text(oc_names, oc_gender, page_text, base=True):\n",
    "    # Use regexes to replace with pronouns\n",
    "    pronouns = {\n",
    "        \"M\": (\"he\", \"him\", \"his\"),\n",
    "        \"F\": (\"she\", \"her\", \"her\"),\n",
    "        \"X\": (\"they\", \"them\", \"their\")\n",
    "    }\n",
    "    subject_pronoun, object_pronoun, possessive_pronoun = pronouns[oc_gender]\n",
    "\n",
    "    # Do replacement in order of names, since full name is first\n",
    "    return_text = page_text\n",
    "    for oc_name in oc_names:\n",
    "        name_to_subject_upper_regex = re.compile(\n",
    "            r\"(^|[\\.!?] |(?:And|But|Or|Yet|So|Because|However|Therefore|Thus|Nevertheless|Nonetheless|Who|What|When|Where|Why|How|Now) )\\b\" + re.escape(oc_name) + r\"\\b\", re.MULTILINE\n",
    "        )\n",
    "        name_to_subject_lower_regex = re.compile(\n",
    "            r\"(^|[,:;] |[,;] (?:for|and|nor|but|or) |(?:yet|so|because|however|therefore|thus|nevertheless|nonetheless|who|what|when|where|why|how|day) )\\b\" + re.escape(oc_name) + r\"\\b\", re.MULTILINE\n",
    "        )\n",
    "        name_to_object_regex = re.compile(\n",
    "            r\"\\b\" + re.escape(oc_name) + r\"\\b\", re.MULTILINE\n",
    "        )\n",
    "        name_to_possessive_upper_regex = re.compile(\n",
    "            r\"(^|[\\.!?] )\\b\" + re.escape(oc_name) + r\"'s?\\b\", re.MULTILINE\n",
    "        )\n",
    "        name_to_possessive_lower_regex = re.compile(\n",
    "            r\"([\\w,:;] )\\b\" + re.escape(oc_name) + r\"'s?\\b\", re.MULTILINE\n",
    "        )\n",
    "        return_text = name_to_possessive_upper_regex.sub(r\"\\1\" + possessive_pronoun.title(), return_text)\n",
    "        return_text = name_to_possessive_lower_regex.sub(r\"\\1\" + possessive_pronoun, return_text)\n",
    "        return_text = name_to_subject_upper_regex.sub(r\"\\1\" + subject_pronoun.title(), return_text)\n",
    "        return_text = name_to_subject_lower_regex.sub(r\"\\1\" + subject_pronoun, return_text)\n",
    "        return_text = name_to_object_regex.sub(object_pronoun, return_text)\n",
    "    \n",
    "    # Do the same for any other names that may appear in text recursively (if base is True)\n",
    "    if base:\n",
    "        other_names_man = [name for name in names_man if name in return_text]\n",
    "        other_names_woman = [name for name in names_woman if name in return_text]\n",
    "        if len(other_names_man) > 0: return_text = generalize_oc_text(other_names_man, \"M\", return_text, base=False)\n",
    "        if len(other_names_woman) > 0: return_text = generalize_oc_text(other_names_woman, \"F\", return_text, base=False)\n",
    "    \n",
    "    return return_text\n",
    "\n",
    "for category in (\"Good\", \"Neutral\", \"Evil\"):\n",
    "    driver.get(base_url.format(category))\n",
    "    time.sleep(1+2*random.random())\n",
    "\n",
    "    category_new_pages = True\n",
    "\n",
    "    while category_new_pages:\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        for link in soup.select(\"a.category-page__member-link\"):\n",
    "            directory_page_url = driver.current_url\n",
    "\n",
    "            page_name = link.get_text().strip()\n",
    "\n",
    "            # Open the link if applicable\n",
    "            link_href = link.get(\"href\", None)\n",
    "            if page_name.startswith(\"File:\") or link_href is None:\n",
    "                continue\n",
    "            link_element = driver.find_element(By.XPATH, f\"//a[@href='{link_href}']\")\n",
    "\n",
    "            # Move page around if can't click\n",
    "            try:\n",
    "                link_element.click()\n",
    "            except (ElementClickInterceptedException, ElementNotInteractableException) as e:\n",
    "                driver.get(domain + link_href)\n",
    "            \n",
    "            child_soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "            oc_names = get_oc_names(page_name)\n",
    "            all_page_text = html2text_handler.handle(\n",
    "                \"\".join(str(p) for p in child_soup.select(\"div#mw-content-text p\"))\n",
    "            )\n",
    "            oc_gender = infer_oc_gender(oc_names, all_page_text)\n",
    "            \n",
    "            desc_text = \"\\n\".join(\n",
    "                p_text for p in child_soup.select(\"div#mw-content-text p\")\n",
    "                if filter_oc_paragraphs((p_text := prepare_scraped_text(\n",
    "                    html2text_handler.handle(str(p))\n",
    "                )))\n",
    "            )\n",
    "            adjusted_desc_text = generalize_oc_text(oc_names, oc_gender, desc_text)\n",
    "\n",
    "            if oc_gender == \"M\":\n",
    "                bodies_man.append(adjusted_desc_text)\n",
    "            elif oc_gender == \"F\":\n",
    "                bodies_woman.append(adjusted_desc_text)\n",
    "            else:\n",
    "                bodies_nb.append(adjusted_desc_text)\n",
    "\n",
    "            time.sleep(1+2*random.random())\n",
    "            driver.get(directory_page_url)\n",
    "            time.sleep(1+2*random.random())\n",
    "    \n",
    "        try:\n",
    "            next_link = driver.find_element(By.XPATH, \"//a[contains(@class, 'category-page__pagination-next')]\")\n",
    "            next_link.click()\n",
    "        except (ElementClickInterceptedException, ElementNotInteractableException) as e:\n",
    "            next_link_element = soup.select_one(\"a.category-page__pagination-next\")\n",
    "            next_link_href = link.get(\"href\", None)\n",
    "            if next_link_href is None:\n",
    "                category_new_pages = False\n",
    "            else:\n",
    "                driver.get(domain + next_link_href)\n",
    "        except NoSuchElementException:\n",
    "            # Leave loop when we can't go to next page anymore\n",
    "            category_new_pages = False\n",
    "        finally:\n",
    "            time.sleep(1+2*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(corpus_path, \"ocdescriptions.m.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(bodies_man))\n",
    "with open(os.path.join(corpus_path, \"ocdescriptions.f.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(bodies_woman))\n",
    "with open(os.path.join(corpus_path, \"ocdescriptions.x.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(bodies_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_scraped_text(os.path.join(corpus_path, \"ocdescriptions.m.txt\"))\n",
    "cleanup_scraped_text(os.path.join(corpus_path, \"ocdescriptions.f.txt\"))\n",
    "cleanup_scraped_text(os.path.join(corpus_path, \"ocdescriptions.x.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colors Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"A-F\", \"G-M\", \"N-Z\"]\n",
    "\n",
    "color_triplets = {}\n",
    "for cat in CATEGORIES:\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_colors:_\" + cat\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    for tr in soup.find_all(\"table\", class_=\"wikitable\")[0].find_all(\"tr\"):\n",
    "        raw_color_name = tr.find_all(\"th\")[0].get_text().lower()\n",
    "        color_hex = \"\"\n",
    "        if tr.find_all(\"td\"):\n",
    "            color_hex = tr.find_all(\"td\")[0].get_text()[1:]\n",
    "        if color_hex != \"\":\n",
    "            color_r = int(color_hex[0:2], 16)\n",
    "            color_g = int(color_hex[2:4], 16)\n",
    "            color_b = int(color_hex[4:6], 16)\n",
    "            color_name = unidecode(raw_color_name).strip()\n",
    "\n",
    "            first_parenthetical_match = re.search(r\"\\((.*?)\\)\", color_name)\n",
    "            if first_parenthetical_match:\n",
    "                reduced_color_name = re.sub(r\"\\s+\\(.*?\\)\", \"\", color_name, count=1)\n",
    "                first_parenthetical = first_parenthetical_match.groups()[0]\n",
    "                if first_parenthetical == \"ncs\":\n",
    "                    color_name = \"natural \" + reduced_color_name\n",
    "                elif first_parenthetical == \"crayola\":\n",
    "                    color_name = reduced_color_name + \" crayon\"\n",
    "                elif first_parenthetical == \"pantone\":\n",
    "                    color_name = reduced_color_name + \" paint\"\n",
    "                elif first_parenthetical in (\"pigment\", \"process\"):\n",
    "                    color_name = \"pigment \" + reduced_color_name\n",
    "                elif first_parenthetical == \"dye\":\n",
    "                    color_name = \"rich \" + reduced_color_name\n",
    "                elif first_parenthetical == \"munsell\":\n",
    "                    color_name = \"brilliant \" + reduced_color_name\n",
    "                elif first_parenthetical in (\"dark\", \"light\", \"metallic\"):\n",
    "                    color_name = first_parenthetical + \" \" + reduced_color_name\n",
    "                elif first_parenthetical in (\"fogra29\", \"fogra39\", \"traditional\"):\n",
    "                    color_name = reduced_color_name\n",
    "            \n",
    "            # Resolve collisions in color list, will need to be resolved manually\n",
    "            if color_name in color_triplets:\n",
    "                color_index = 0\n",
    "                while f\"{color_name} {color_index}\" in color_triplets:\n",
    "                    color_index += 1\n",
    "                color_name = f\"{color_name} {color_index}\"\n",
    "\n",
    "            color_triplets[color_name] = (color_r, color_g, color_b)\n",
    "    \n",
    "    time.sleep(1.5)\n",
    "\n",
    "with open(os.path.join(data_path, \"colors.general.txt\"), \"w\") as f:\n",
    "    for color_name, color_triplet in sorted(color_triplets.items(), key=lambda item: item[0]):\n",
    "        color_r, color_g, color_b = color_triplet\n",
    "        f.write(f\"{color_r:3d},{color_g:3d},{color_b:3d}:{color_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-sort the colors after removing some manually\n",
    "with open(os.path.join(data_path, \"colors.general.txt\")) as f:\n",
    "    data = [(line.split(\":\")[1], line) for line in f.readlines()]\n",
    "data.sort(key=lambda row: row[0])\n",
    "backup_file(os.path.join(data_path, \"colors.general.txt\"))\n",
    "with open(os.path.join(data_path, \"colors.general.txt\"), \"w\") as f:\n",
    "    for name, line in data:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e2f38b81313eadf8daa7f617b40a5f79fe584784fafddc9bd8f14fd36d262b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
